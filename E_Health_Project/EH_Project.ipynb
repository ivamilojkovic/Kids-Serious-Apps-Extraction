{"cells":[{"cell_type":"markdown","metadata":{"id":"a9fEf8vRfmHK"},"source":["# **USEFUL LIBRARIES**"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12665,"status":"ok","timestamp":1639411512415,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"sqME1FS5ftBE","outputId":"bb1e73d2-afa9-4b96-e71b-0a9644c46327"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import  urllib.request\n","import urllib.parse\n","from bs4 import BeautifulSoup\n","\n","\n","import tinydb\n","from langdetect import detect\n","from pymed import PubMed\n","\n","import requests\n","import re\n","import random\n","import glob\n","import os\n","import time\n","\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"mveDZqjcBWNW"},"source":["# **UTILITY METHODS**\n","\n","* get_publication_ID\n","* create_list_of_keywords\n","* text2BoW"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":807,"status":"ok","timestamp":1639411793024,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"juzUZT2jBche"},"outputs":[],"source":["def extract_publication_info(publication):\n","    info = publication.toDict() #each element of the papers list is a dictionary class\n","    ID = info[\"pubmed_id\"]\n","    if len(ID)>10:\n","        ID = ID.split('\\n')\n","        ID = ID[0]\n","    title = info['title']\n","    if 'authors' in info:\n","      authors = info['authors']\n","    else:\n","      authors = []\n","    if 'keywords' in info:\n","      keywords = info['keywords']\n","    else:\n","      keywords = []\n","    if 'journal' in info:\n","      journal = info['journal']\n","    else:\n","      journal = []\n","    if 'doi' in info:\n","      doi = info['doi']\n","    else:\n","      doi = []\n","\n","    return ID, title, authors, keywords, journal, doi\n","\n","\n","def create_list_of_keywords(filename, keyword_or_evidence = 'e'):\n","  fp = open(filename, 'r')\n","  lines = fp.readlines()\n","  kw_list = []\n","\n","  for line in lines:\n","    if keyword_or_evidence == 'e':\n","      x = line.split('|')\n","      x = x[1]\n","      x = x.split('\\n')\n","    else:\n","      x = line.split('\\n')\n","    kw_list.append(x[0])\n","  \n","  fp.close()\n","  return kw_list\n","\n","def text2BoW(processed_docs):\n","\n","  # Converting text to Bag of Words (BoW)\n","  dictionary = gensim.corpora.Dictionary(processed_docs)\n","  # Remove very rare and very common words\n","  #dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n = 100000)\n","  # Create the Bag-of-words for each abstract reporting how many words \n","  # and how many times those words appear\n","  bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","\n","  return dictionary, bow_corpus\n","\n","def study_type_keywords(file_path):\n","\n","  studytype_dict = {}\n","\n","  # Go to all the folders that have names from 1 to 7\n","  for file_name in range(1,8):\n","    file_path_2 = os.path.join(file_path, str(file_name))\n","    all_txt_files = os.listdir(file_path_2)\n","\n","    # Go through all txt files in the folder\n","    for txt_file in all_txt_files:\n","\n","      # Take the name without .txt extension\n","      txt_file_name = txt_file[:-4]\n","      # If this key doesn't exist add it and assign it with an empty list\n","      if txt_file_name not in studytype_dict:\n","        studytype_dict[txt_file_name] = []\n","\n","      txt_file_path = os.path.join(file_path_2, txt_file)\n","\n","      # Read the lines from one .txt file\n","      with open(txt_file_path) as f:\n","        lines = f.readlines()\n","      \n","      # Take one line and append it to the list\n","      for line in lines:\n","        if line[:2] == '--':           # Some .txt files start with --SMTH--\n","          continue\n","        # Remove '\\n' from each line\n","        line = line[:-2]\n","        # If the keyword doesn't exist in the list add it\n","        if line not in studytype_dict[txt_file_name]:\n","          studytype_dict[txt_file_name].append(line.lower())\n","\n","  return studytype_dict\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"y1nc2HfBb36m"},"source":["# **PUBLICATION CLASS**\n","\n","This class containes following methods:\n","* display_abstract()\n","* count_overall_occuracies()\n","* KeyWords_found_in_abstract()\n","* number_of_different_keywords\n","* study_quality\n","\n","The publication is defined by its **url** address. We are using only the publications' **abstracts** since they are always available in **PubMed** database. We have two textual files that contain **keywords** which we used to score the publication according to its connection with our interest domain. We would like to give higher scores to the articles that are about serious applications for kids.   "]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":359,"status":"ok","timestamp":1639411512764,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"X71RbRHrbP9d"},"outputs":[],"source":["class Publication:\n","  \n","  def __init__(self, in_url):\n","\n","    \"\"\" Takes URL of the publication, takes its abstract in the original \n","        format, extracts only the useful text from the abstract.  \"\"\" \n","    \n","    self.url = in_url\n","\n","    # Given the URL we extract the abstract in the original format\n","    try:\n","      page = urllib.request.urlopen(self.url)\n","      time.sleep(6)\n","    except requests.exceptions.ConnectionError:\n","      r.status_code = \"Connection refused\"\n","    \n","    soup = BeautifulSoup(page, 'lxml')\n","    article_abs = soup.find(id = \"abstract\")\n","\n","    # Sometimes we can't find the publication -> print '-'\n","    try:\n","      self.abstract = article_abs.get_text()\n","    except AttributeError:\n","      self.abstract = ''\n","      print('-')\n","\n","    # Take only the useful text from the abstract -> remove its subparts, '\\n' signs... \n","    self.extract_abstract_text()\n","\n","    ####################### Check if this is useful ############################\n","    self.EvidenceLevel = create_list_of_keywords('keywords_studydesign.txt')\n","    self.KeyWords = create_list_of_keywords('keywords_context.txt', 'k')\n","    ############################################################################\n","\n","  def display_abstract(self): \n","    print(self.abstract)\n","\n"," \n","  def count_overall_occurancies(self):\n","\n","    \"\"\" Takes our manually selected keywords and counts their occurances \n","        in the abstract. Returns the sum of all occurances.  \"\"\"\n","    \n","    abs_txt = self.abstract\n","    # Convert all big letters into low letters\n","    abs_txt = abs_txt.lower()  \n","    sum_txt = 0\n","    for word in self.KeyWords:\n","        count = abs_txt.count(word)\n","        sum_txt += count\n","    return(sum_txt)\n","  \n","  \n","  def KeyWords_found_in_abstract(self):\n","    \n","    \"\"\" Takes our manually selected keywords and checks their existence in \n","        the abstract. Returns the list of founded keywords.  \"\"\"\n","  \n","    abs_txt = self.abstract\n","    kw = []\n","    for word in self.KeyWords:\n","        if word in abs_txt.lower():\n","            kw.append(word)\n","    return(kw)\n","\n","  \n","  def number_of_different_keywords(self):\n","    return(len(self.KeyWords_found_in_abstract()))\n","\n","  \n","  def study_quality(self):\n","    abs_txt = self.abstract\n","    abs_txt = abs_txt.lower()\n","    sd = []\n","    for word in self.EvidenceLevel:\n","      word_low = word.lower()\n","      if word in abs_txt or word_low in abs_txt:\n","        sd.append(word)\n","    return(sd)\n","\n","  \n","  def study_score(self):\n","    total_num = self.number_of_different_keywords()\n","    sd = self.study_quality()\n","    score = round(len(sd)/(total_num + 1),5)\n","    return score\n","\n","  \n","  def manual_score(self):\n","    total_num = self.number_of_different_keywords()\n","    kw = self.KeyWords_found_in_abstract()\n","    score = round(len(kw)/(total_num + 1),5)\n","    return score\n","\n","  \n","  def extract_abstract_text(self):\n","\n","    #print('Abstract before: \\n')\n","    #print(self.abstract)\n","    #print('\\n')\n","\n","    # Remove all \\n signs\n","    self.abstract = self.abstract.replace('\\n', '') \n","    # Remove the Abstract word in the beginning of each article\n","    self.abstract = self.abstract.replace('Abstract', '')\n","    # Remove multiple spaces if they exist\n","    self.abstract = \" \".join(self.abstract.split())\n","\n","    # Remove the Keywords section\n","    self.abstract = re.sub(r\"(?i)(?:Keywords:).*?[.?!]\", '', self.abstract)\n","\n","    # Find subparts such are Conclusion, Methods... and remove them\n","    subparts = self.find_article_subparts()\n","    for s in subparts:\n","      self.abstract = self.abstract.replace(s, '')\n","\n","    # Remove all types of url addresses\n","    self.abstract = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', self.abstract)\n","\n","    # Remove the space before the first string\n","    self.abstract.lstrip()\n","    \n","    #print('Abstract after: \\n')\n","    #print(self.abstract)\n","\n","  def get_keywords(self):\n","    self.keywords = re.findall(r\"(?i)(?:Keywords:).*?[.?!]\", self.abstract)\n","    if len(self.keywords)==0:\n","      self.keywords = []\n","    else:\n","      self.keywords = self.keywords[0]\n","      self.keywords = self.keywords.split(';')\n","      self.keywords[0] = self.keywords[0].replace('Keywords:', '')\n","    return self.keywords\n","\n","  def find_article_subparts(self):\n","    subparts = re.findall(r'(\\w+:)', self.abstract)\n","    return subparts\n","\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"e_lNVE0fdHsb"},"source":["# **PUBLICATIONS CLASS**"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1639411512993,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"1pGfH-IgdIZN"},"outputs":[],"source":["class PubMedPublications:\n","\n","   def __init__(self, number_of_articles = 10,             \n","                tool = 'ehealth_group14', \n","                pubmedurlbase = \"https://pubmed.ncbi.nlm.nih.gov/\",\n","                email = \"iva97.ja@gmail.com\"):\n","    \n","    self.tool = tool\n","    self.email = email\n","    self.num_articles =  number_of_articles\n","    self.pubmedurlbase = pubmedurlbase\n","\n","    self.pubmed = PubMed(tool = self.tool, email = self.email)\n","    self.abstracts = []\n","    self.article_dict = {}\n","    self.abs_IDs = set()\n","\n","   def search(self, query_words):\n","\n","     # Initialize empty lists for all info\n","     IDs = []\n","     titles = []\n","     kws = []\n","     journals = []\n","     dois = []\n","     authorss = []\n","     \n","     for query_word in query_words:\n","\n","       # Get best N articles for given query\n","       try:\n","         n_article_results = self.pubmed.query(query_word, max_results = self.num_articles)  \n","       except TypeError:\n","         pass\n","       time.sleep(10)\n","      \n","       for one_article_result in n_article_results:\n"," \n","         # Get article INFO and abstract text\n","         article_ID, title, authors, keywords, journal, doi = extract_publication_info(one_article_result)\n","\n","         # Do this only if this ID is new\n","         if article_ID not in self.abs_IDs:\n","           self.abs_IDs.add(article_ID)\n","           IDs.append(article_ID)\n","           titles.append(title)\n","           kws.append(keywords)\n","           journals.append(journal)\n","           dois.append(doi)\n","           authorss.append(authors)\n","\n","           article_abstract = Publication(self.pubmedurlbase + article_ID + \"/\")\n","           \n","\n","           ########################## Check this part ############################\n","\n","           # Occurance of all keywords in the article \n","           all_kw_occurance = article_abstract.count_overall_occurancies()\n","           # Keywords that appear in the article\n","           kw = article_abstract.KeyWords_found_in_abstract()\n","           # Number of keywords that don't appear in the article\n","           nb_diff = article_abstract.number_of_different_keywords()\n","           # Article score\n","           sd = article_abstract.manual_score()\n"," \n","           #######################################################################\n","\n","           # Dictionary that conatines abstract ID and its score\n","           if article_ID not in self.article_dict:\n","             self.article_dict[article_ID] = sd\n","\n","             # Add that abstract text to the global list of abstracts\n","             self.abstracts.append(article_abstract.abstract)\n","     \n","     return self.abstracts, IDs, titles, kws, journals, dois, authorss\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6IjgqvuddGtI"},"source":["# **MAIN**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"executionInfo":{"elapsed":82350,"status":"error","timestamp":1639368335335,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"kKR0gt4ljbqZ","outputId":"9b9dd4e1-3fa4-453b-d6fd-fd7f173c0c41"},"outputs":[],"source":["# Make a dictionary of articles IDs and thier scores for the given query, as well\n","# as the list of partialy-processed abstract texts\n","all_publications = PubMedPublications(number_of_articles = 20)\n","queries = ['apps kids', 'serious games', 'brain kids apps', 'school games',\n","           'teaching', 'game mechanisms', 'learning', 'study skills' ]\n","all_publications.search(query_words = queries)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9_IX8r7oY-g"},"outputs":[],"source":["# Shuffle the abstracts for better generalization\n","random.shuffle(all_publications.abstracts)\n","\n","# Split the abstracts into training and test sets\n","dataset_size = len(all_publications.abstracts)\n","train_size = int(dataset_size*0.7)\n","test_size = dataset_size - train_size\n","\n","train_abstracts = all_publications.abstracts[:train_size]\n","test_abstracts =  all_publications.abstracts[train_size:]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpjWOi6gpzum"},"outputs":[],"source":["# Example of one partialy-processed abstract text\n","all_publications.abstracts[0]"]},{"cell_type":"markdown","metadata":{"id":"yU1Bimfqvgph"},"source":["# *Text Information Extraction & Topic Modelling*\n","\n","Useful links:\n","* https://blog.aureusanalytics.com/blog/5-natural-language-processing-techniques-for-extracting-information\n","* https://towardsdatascience.com/something-from-nothing-use-nlp-and-ml-to-extract-and-structure-web-data-3f49b2f72b13"]},{"cell_type":"markdown","metadata":{"id":"Yx5c1lK4imUC"},"source":["# **LDA** - Latent Dirichlet Allocation\n","\n","Useful links:\n","* https://towardsdatascience.com/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc\n","* https://www.youtube.com/watch?v=HnnKHP6s-n0\n","* https://towardsdatascience.com/clustering-documents-with-python-97314ad6a78d\n","* https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n","* https://www.freecodecamp.org/news/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164/\n","* https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5467,"status":"ok","timestamp":1639366470527,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"MjUahvig5ku9","outputId":"e10f7ab3-252d-4e70-de0a-878e2ea86f0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to C:\\Users\\Phillip\n","[nltk_data]     Maya\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["\n","\n","import nltk\n","nltk.download('wordnet')\n","from nltk import tokenize\n","from nltk import PorterStemmer, WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","from tqdm.notebook import tqdm\n","tqdm.pandas()\n","\n","import gensim\n","from gensim import corpora, models\n","from gensim.utils import tokenize\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1639411523926,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"Ax-00xQdi8Ps"},"outputs":[],"source":["class TextPreprocessing:\n","\n","  def __init__(self, abs_text):\n","    self.abs_text = abs_text\n","\n","  def preprocess(self):\n","    self.split_abstract_into_words()    \n","    self.remove_short_words()   \n","    self.remove_stop_words()   \n","    self.lemmatize_stemming()\n","\n","    #print(self.tokenized_words)\n","    #print(self.words)\n","    #print(self.words_without_stop)\n","    #print(self.words_final_form)\n","\n","    return self.words_final_form\n","\n","  def split_abstract_into_words(self):\n","    self.tokenized_words = list(gensim.utils.tokenize(self.abs_text))\n","\n","  def remove_short_words(self, thresh = 3):\n","    self.words = []\n","    for word in self.tokenized_words:\n","      if len(word)>=thresh:\n","        self.words.append(word)\n","\n","\n","  def remove_stop_words(self):\n","    self.words_without_stop = []\n","    for w in self.words:\n","      if w not in gensim.parsing.preprocessing.STOPWORDS:\n","        self.words_without_stop.append(w)\n","\n","\n","  def lemmatize_stemming(self):\n","    stemmer = PorterStemmer()\n","    self.words_final_form = []\n","    for w in self.words_without_stop:\n","      new_w = stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v'))\n","      self.words_final_form.append(new_w)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"elapsed":258,"status":"error","timestamp":1639371351237,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"VOdtXsFtPBSm","outputId":"54956fd3-f854-4c79-d248-cd1fff0d5636"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-e75765a2ebdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Go through all of the abstracts and preprocess them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprocessed_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_abstracts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprocessing_abs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_abs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_abstracts' is not defined"]}],"source":["# Go through all of the abstracts and preprocess them\n","processed_docs = []\n","for abstract in train_abstracts:\n","  processing_abs = TextPreprocessing(abstract)\n","  result = processing_abs.preprocess()\n","  processed_docs.append(result)\n","\n","#Create a dictionary from 'processed_docs' containing the number of times a \n","# word appears in all of the abstracts (training dataset)\n","dictionary, bow_corpus = text2BoW(processed_docs)\n","\n","\n","\n","# Define the LDA model - suppose the number of topics (here it is 8)\n","LDA_model =  gensim.models.LdaMulticore(bow_corpus, \n","                                        num_topics = 5, \n","                                        id2word = dictionary,                                    \n","                                        passes = 10,\n","                                        workers = 2)\n","\n","# For each topic, we will explore the words occuring in that topic \n","# and its relative weight\n","for idx, topic in LDA_model.print_topics(-1):\n","    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"4oKwIMAPhShi"},"source":["# **Abstract Classifocation Topics**\n","\n","\n","1.   Class 1\n","2.   Class 2\n","3.   Class 3\n","4.   Class 4\n","5.   Class 5\n","6.   Class 6\n","7.   Class 7\n","8.   Class 8\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2L9xqRVqiFr8"},"source":["# **Classification on New Articles**\n","\n","The highest score refers to which class the article belongs. Here we can use new articles to evaluate the model trained on the training set previously defined. Since the problem is not supervised we had to analyse these 8 classes and give them proper names. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmHaXzDHhRxB"},"outputs":[],"source":["processing_abs = TextPreprocessing(test_abstracts[0])\n","result = processing_abs.preprocess()\n","\n","# Data preprocessing step for the unseen document\n","bow_vector = dictionary.doc2bow(result)\n","\n","\n","for index, score in sorted(LDA_model[bow_vector], key=lambda tup: -1*tup[1]):\n","    print(\"Score: {}\\t Topic: {}\".format(score, LDA_model.print_topic(index, 5)))"]},{"cell_type":"markdown","metadata":{"id":"HQxGqsbfu07c"},"source":["# **VISUALISATION**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GugQSN7_u5yF"},"outputs":[],"source":["%matplotlib inline\n","!pip install pyLDAvis\n","import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LO_kJNn-wbdi"},"outputs":[],"source":["vis = gensimvis.prepare(topic_model=LDA_model, corpus=bow_corpus, dictionary=dictionary)\n","pyLDAvis.enable_notebook()\n","pyLDAvis.display(vis)"]},{"cell_type":"markdown","metadata":{"id":"5wwegjS9VHR9"},"source":["# **STUDY TYPE CLASSIFICATION**\n","\n","Given the \"*Study Type Dictionary*\" folder which containes keywords for each of the following 8 study types: \n","* *MetaAnalysis.txt*\n","* *ObservationalStudy.txt*\n","* *RCT.txt*\n","* *SystematicReview.txt*\n","* *CaseControl.txt*\n","* *CaseSeries.txt*\n","* *CohortStudy.txt*\n","* *Other.txt*\n","\n","\n","There are *7* folders where each contains aforementioned textual files and many of keywords are repeat when comparing e.g. the *MetaAnalysis.txt* from the folders *1* and *2*. The idea is to go through all of this folders and files and gather unique keywords that will represnt each one of the study types.Looking at the provided keys, they are already lemmatized. Therefore, we don't need to preprocess them too.\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6208,"status":"ok","timestamp":1639411537081,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"so2pxlchVuBo","outputId":"435cd1cd-705e-4fff-afc9-48d56bd35806"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dictionary keys:\n","dict_keys(['MetaAnalysis', 'ObservationalStudy', 'RCT', 'SystematicReview', 'Other']) \n","\n","MetaAnalysis keywords:\n","['engagement inde', 'cross-classification analysi', 'pooled sm', '4-level kirkpatrick mode', 'cross-sectional surve', 'metaprop random effects analysi', 'metaprop fix effect analysi', 'meta-analytic revie', 'review of published studie', 'meta-ethnograph', 'meta-analysi', 'meta synthese', 'meta descriptio', 'meta', 'meta-analyse', 'meta-analyti', 'meta analyse', 'meta analyti', 'meta-descriptio', 'meta-synthese', 'meta-evaluatio', 'meta evaluatio', 'meta analysi', 'multivariate analysi', 'hypothesis test ', 'statistical tes', 'evaluation stud', 'statistical method', 'synthesize dat', 'evaluate dat', 'regression analys', 'cross-classification analysi', 'cross-sectiona', 'meta-analytic revie', 'review of published studie', 'meta-ethnograph', 'meta-analysi', 'meta-analysi', 'meta synthese', 'meta descriptio', 'meta-analyse', 'meta-analyse', 'meta-analyti', 'meta analyse', 'meta analyti', 'meta-descriptio', 'meta-synthese', 'meta-evaluatio', 'meta evaluatio', 'meta analysi', 'multivariate analysi', 'statistic', 'statistical tes', 'evaluation stud', 'statistical method', 'synthesize dat', 'effect siz', 'exclusion criteri', 'meta-regressi', 'cross-classification analysi', 'pooled sm', '4-level kirkpatrick mode', 'meta synthese', 'meta descriptio', 'meta', 'meta-analyse', 'meta-analyti', 'meta analyse', 'meta analyti', 'meta-descriptio', 'meta-synthese', 'meta-evaluatio', 'meta evaluatio', 'meta analysi', 'multivariate analysi', 'hypothesis test ', 'statistical tes', 'regression analys', 'cross-classification analysi', 'pooled sm', '4-level kirkpatrick mode', 'meta synthese', 'meta descriptio', 'meta', 'meta-analyse', 'meta-analyti', 'meta analyse', 'meta analyti', 'meta-descriptio', 'meta-synthese', 'meta-evaluatio', 'meta evaluatio', 'meta analysi', 'multivariate analysi', 'hypothesis test ', 'statistical tes', 'regression analys', 'cross-classification analysi', 'pooled sm', '4-level kirkpatrick mode', 'meta synthese', 'meta descriptio', 'meta', 'meta-analyse', 'meta-analyti', 'meta analyse', 'meta analyti', 'meta-descriptio', 'meta-synthese', 'meta-evaluatio', 'meta evaluatio', 'meta analysi', 'multivariate analysi', 'hypothesis test ', 'statistical tes', 'regression analys', 'cross-classification analysi', 'pooled sm', '4-level kirkpatrick mode', 'meta synthese', 'meta descriptio', 'meta', 'meta-analysi', 'meta-analyti', 'meta analysi', 'meta analyti', 'meta-descriptio', 'meta-synthese', 'meta-evaluatio', 'meta evaluatio', 'meta analysi', 'multivariate analysi', 'hypothesis test ', 'statistical tes', 'regression analys', 'cross-classification analysi', 'pooled sm', '4-level kirkpatrick mode', 'meta synthese', 'meta descriptio', 'meta', 'meta-analyse', 'meta-analyti', 'meta analyse', 'meta analyti', 'meta-descriptio', 'meta-synthese', 'meta-evaluatio', 'meta evaluatio', 'meta analysi', 'multivariate analysi', 'hypothesis test ', 'statistical tes', 'regression analys'] \n","\n","Number of study types: 5\n"]}],"source":["#!unzip \"Study type dictionaries\"\n","\n","path = \"StudyType_Dictionaries\"\n","study_type_dictionary = study_type_keywords(path)\n","\n","# The keys in the dictonary\n","print('Dictionary keys:')\n","print(study_type_dictionary.keys(), '\\n')\n","\n","# List of keywords for one key e.g. MetaAnalysis\n","print('MetaAnalysis keywords:')\n","print(study_type_dictionary['MetaAnalysis'],'\\n')\n","\n","# Number of classes\n","print('Number of study types: {}'.format(len(study_type_dictionary.keys())))"]},{"cell_type":"markdown","metadata":{"id":"Nk-Cor8BFQPh"},"source":["## *Publications for DataBase Apps*\n","\n","Now, we want to go through all of the games we have in our database. Taking the games' names we will query the PubMed database and extract couple of publications for each one of them. When the publications are collected, they will be preprocessed as we did before in the code. Finally, in each of the abstract we will count the occurances of keywords for each study plan independently. The result will be the perentage of abstract belonging to one of the classes that represent particular study type. The abstract will be assigned with a class for which it has the highest probability of belonging to."]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1639411537082,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"u_SWTDXfFbsd"},"outputs":[],"source":["from tinydb import TinyDB\n","db = TinyDB('Ourdatabase.json')"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":953,"status":"ok","timestamp":1639411538029,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"AeMExHflYXPv","outputId":"8779ab54-56fe-4fe3-887d-459437d5c93e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Game names: \n"," ['Udemy - Online Courses', 'Learn 33 Languages Free - Mondly', 'Yandex.Translate', 'Star Chart', 'Simpler — выучить английский язык проще простого', 'Khan Academy', 'ELSA - Learn English Speaking', 'Aprender Inglés Gratis!', 'Sololearn: Learn to Code (Python, Javascript, etc)', 'PlantNet Plant Identification', 'Toca Kitchen', 'Quizizz: Play to learn', 'U-Dictionary: Translate Now', 'Current Affairs 2020 General Knowledge Quiz', 'How to Draw - Easy Lessons', 'Peak – Brain Games & Training', 'NCERT Books', 'ClasseViva Famiglia', 'ANTON: Kindergarten - Grade 5', 'Edmodo', 'Solar System Scope', 'Language Learning - Spanish, Korean, French & More', 'myCBSEguide - CBSE Papers & NCERT Solutions', 'Meritnation: CBSE, ICSE & more (Free Live Classes)', \"BYJU'S – The Learning App\", 'شعلة - درّب عقلك يومياً', 'Hello English: Learn English', 'Remind: School Communication', 'Simply Piano by JoyTunes', 'Learn English Words Free', 'EWA: Learn English & Spanish Language', 'Vedantu: LIVE Learning App | Class 1-12, JEE, NEET', 'Экзамен ПДД 2021: Билеты ГИБДД', 'ClasseViva Studenti', 'Photomath', 'Learn English Phrases | English Translator', 'Scientific calculator plus advanced 991 calc', 'Learn English with ABA English – Study English', 'NCERT Solutions of NCERT Books', 'Cake: Free expressions updated daily!', 'Math Tricks', 'Latest Current Affairs & GK in English & Hindi', 'Learn Japanese Language -Drops', 'Division calculator', 'Busuu: Learn Languages', 'Dich Tieng Anh TFlat Translate', 'Superbook Kids Bible, Videos & Games (Free App)', 'Learn Chinese - ChineseSkill', 'TO-FU Oh!SUSHI', 'ClassDojo', 'Mathway: Scan Photos, Solve Problems', 'Duolingo: language lessons', 'Exam Preparation: Live Classes', 'Lingokids - kids playlearning™', 'Tandem: Learn & speak language', 'NASA', 'DailyArt - Daily Dose of Art', 'Current Affairs GK for SSC, Railways, Banking, IAS', 'Periodic Table 2021 - Chemistry', 'Canvas Student', 'Exam Preparation : Live Class, Videos, Mock Tests', 'Brainly – Home Learning & Homework Help', 'Andy - English Speaking Bot', 'Unacademy Learner App', 'Star Walk 2 Ads+ Sky Map View', 'Learn Korean Language by Drops', 'HelloChinese: Learn Chinese', 'PictureThis - Plant Identifier', 'School Planner', 'JW Language', 'Physics Wallah', 'English Grammar Test', 'TED', 'تعليم اللغة الانجليزية من الصفر بالصوت والصورة', 'Lamsa: Child Early Education & Development Program', 'Билеты ПДД 2021+Экзамен ПДД', 'Elevate - Brain Training Games', 'Quizlet: Learn Languages & Vocab with Flashcards', 'Programming Hub: Learn to code', 'Learn German. Speak German', 'wifistudy - #1 Exam Preparation, Free Mock Tests', 'Chestionare Auto DRPCIV', 'Extramarks – The Learning App', 'Oust - Learn Smarter', 'ISS Detector Satellite Tracker', 'Exam Preparation App: Live Class | Mock Test | PYP', 'Sekolah.mu - Eksplorasi Ilmu Untuk Semua', 'Yousician - Guitar, Ukulele, Bass and Singing', 'Ncert Books & Solutions', 'Learn French. Speak French', 'HelloTalk - Learn Languages', 'Wlingua - English Language Course', 'LingoDeer - Learn Languages', 'QANDA: Instant Math Helper', 'Bible App for Kids: Audio & Interactive Stories', 'Buddy.ai: English for kids', 'Ayat - Al Quran', 'Coursera', 'Learn Korean Phrases | Korean Translator', 'Mimo: Learn coding, programming', 'Microsoft Math Solver', 'Careerwill App', 'English with Lingualeo', 'Toppr - Learning App for Class 5 - 12', 'Rosetta Stone: Learn, Practice & Speak Languages', 'iTutor Learning App - NEET/JEE & Class 8-10', 'Skill Academy by Ruangguru', 'Babbel - Learn Languages', 'Kahoot! Play & Create Quizzes', 'Snap Homework App', 'Hi Dictionary-Translate Now', 'ISS Live Now: Live HD Earth View and ISS Tracker', 'Learn Top 300 English Words', 'DROPS Visual Language Learning', 'NeuroNation - Brain Training & Brain Games', 'Miga Town: My World', 'Doubtnut: NCERT Solutions, IIT JEE & NEET App', 'Lumosity: Brain Training', 'Learn English - 15,000 Words', 'Cambly - English Teacher', 'Utkarsh App :  Your Smart E - Learning Solution', 'Left vs Right: Brain Training Games', 'My Town: Police Station game', 'Fluvsies - A Fluff to Luv', 'Baby Games for 2,3,4 year old kids', 'Baby Panda World', 'Meow Meow Star Acres', \"Baby Panda's Fashion Dress Up\", 'Toca Kitchen 2', 'Truck games for kids - build a house, car wash', \"Cooking Mama: Let's cook!\", 'Little Panda Policeman', 'Math Games for the Brain', 'Princess Salon: Frozen Party', 'MentalUP - Learning Games & Brain Games', \"Baby Panda's School Bus\", 'Supermarket: Shopping Games for Kids', \"Baby Panda's Supermarket\", 'My Town: City Builder Game', 'My Town Home Family Doll House', 'Coloring & Learn', 'Make-Up Me: Superstar', 'Toca Life World: Build stories', 'Baby Panda’s Ice Cream Shop', \"Little Panda's Restaurant\", \"Little Panda's Chinese Recipes\", 'Girl Games: Unicorn Cooking Games for Girls Kids', 'Math Games, Learn Add, Subtract, Multiply & Divide', 'Pepi House: Happy Family', 'Do Not Disturb Funny Prankster', 'Skillz - Logic Brain Games', 'Pepi Wonder World: Islands of Magic Life!', 'LEGO® DUPLO® Train', 'Pepi Super Stores: Fun & Games', '知識王', '超級單字王 - 英檢、多益、托福 輕鬆學', 'Animal Jam', 'Pepi Hospital: Learn & Care', 'Smolsies - My Cute Pet House', 'Baby Panda Care', 'Mandala Coloring Pages', 'Masha and the Bear. Educational Games', 'Water Sort - Color Puzzle Game', 'hocus.', 'Royal Match', 'Flow Free: Bridges', 'Doodle Alchemy Animals', 'Charm King', 'Mr Bullet - Spy Puzzles', 'Sporos', 'Family Guy Freakin Mobile Game', 'Bee Brilliant', '애니팡', 'Hangman', 'That Level Again', '2048 Number puzzle game', 'Fruit Nibblers', 'Sudoku', \"Where's My Water? 2\", '애니팡 사천성', 'Ice Crush', 'Toon Blast', 'Pokémon Shuffle Mobile', 'Escape Alcatraz', 'Minesweeper', 'Move the Box', '2048', 'Troll Face Quest: Video Games', 'Puzzle Fuzzle', 'Brain Wash - Thinking Game', 'Find the differences 750 + levels', 'Matches Puzzle Game', 'Nonograms Katana', '2048', 'Puzzledom - classic puzzles all in one', 'Bubble Shooter Rainbow', '100 Doors Games: School Escape', 'My Story - Mansion Makeover', 'Hidden Object Enchanted Castle – Hidden Games', 'Wood Block - Music Box', 'Pull the Pin', 'Find the Differences 500 levels', 'Fruits Forest : Rainbow Apple', 'Candy Valley - Match 3 Puzzle', 'Numpuz: Classic Number Games, Riddle Puzzle', 'Disney Emoji Blitz Game', 'Brain It On! - Physics Puzzles', 'Panda Cube Smash - Big Win with Lucky Puzzle Games', 'Fruit Land – match3 adventure', 'Words', 'Family Zoo: The Story', 'Jewels Classic - Jewel Crush Legend', 'Can You Escape - Adventure', 'Hero Rescue', 'Draw Happy Life - drawing apps', 'Flow Free', 'Mouse', 'Can You Escape 3', 'Logic Master 1 Mind Twist', 'Smart Puzzles Collection', 'Mahjong', 'Sudoku', '1010! Block Puzzle Game', 'Marble Mission', 'Jigty Jigsaw Puzzles', 'Cut the Rope', 'Shootout 3D', 'Cut the Rope: Time Travel', 'Fishdom', 'Sudoku - Classic Sudoku Puzzle', 'LINE Bubble 2', 'One touch Drawing', 'Plumber', 'Cut the Rope: Magic', 'Word Search Puzzle', 'Marble Legend', 'Simon’s Cat Crunch Time - Puzzle Adventure!', 'Color Fill 3D', 'Mahjong', 'Build a Bridge!', 'Puzzle & Dragons', 'Can you escape the 100 room XII', 'Troll Face Quest: TV Shows', 'Dig This!', 'The Room Three', 'Mariam Game', 'Woodoku', 'Monster Busters: Hexa Blast', 'Juice Jam - Match 3 Games', 'Bejeweled Blitz', 'Bubble Shooter: Panda Pop!', 'BlockPuz: Jigsaw Puzzles &Wood Block Puzzle Game', '2 Player Reactor (Multiplayer)', 'Auralux', 'Shoot the Apple', 'Antistress - relaxation toys', 'درب التحدي - العاب ذكاء', 'Block Puzzle Gem: Jewel Blast', 'Disney Frozen Free Fall Games', 'Jewels Jungle : Match 3 Puzzle', 'Emoji Puzzle!', 'Word Search Games in english', 'Block Puzzle Jewel', 'Tower of Savior Guide', 'Construction City 2', '100 Doors Seasons - Puzzle Games. Logic Puzzles.', 'Toy Blast', 'Mahjong', 'Escape Block King', 'ASolver - show me the puzzle, and I will solve it', 'How to Loot - Pin Pull & Hero Rescue', 'Rope Around!', 'Tower of Saviors', 'Troll Face Quest: Video Memes - Brain Game', 'LINE Bubble!', 'Shoot Bubble Deluxe', 'Puzzles with Matches', 'Falling Puzzle®', 'Block! Hexa Puzzle™', 'Harry Potter: Puzzles & Spells', 'Mahjong Journey: Tile Match', 'Difference Find King', 'Crafty Candy - Match 3 Game', 'Pixel Art Book - Color by Number Free Games', 'Can you escape the 100 room VII', 'Indy Cat - Match 3 Puzzle Adventure', 'Pixel Art: color by number', 'Maleficent Free Fall', 'My Home Design - Modern City', 'Magic Jigsaw Puzzles - Game HD', 'Neighbours from Hell: Season 1', 'PixWords™', 'Mazes & More', 'Magic Cube Puzzle 3D', 'Can you escape the 100 room V', 'Brain Out: Can you pass it?', 'Energy: Anti Stress Loops', 'Buttons and Scissors', 'Mind Games', '100 Doors Puzzle Box', 'Love Poly: Rotating Puzzle', 'Bridge Construction Simulator', 'Cut the Rope 2', 'Manor Cafe', \"LINE Pokopang - POKOTA's puzzle swiping game!\", 'Match 3D - Matching Puzzle Game', 'Doodle God HD Free Аlchemy', 'Block Puzzle & Conquer', 'Match To Win: Win Real Cash', 'Dr. Mario World', 'House Paint', 'LINE: Disney Tsum Tsum', '妖怪ウォッチ ぷにぷに', 'Slugterra: Slug it Out 2', '애니팡2', 'CubeX - Cube Solver, Virtual Cube and Timer', 'Matchington Mansion', 'Genies & Gems - Match 3 Game', 'Magnetic balls bubble shoot', 'PUZZLE STAR BT21', 'Drop The Number® : Merge Game', 'DOP: Draw One Part', 'Two Dots', 'Candy Crush Jelly Saga', 'Find Out: Find Hidden Objects!', 'Mahjong Treasure Quest', 'Indy Cat for VK', 'Escape game: 50 rooms 2', 'Mahjong 3', 'Bubble Island 2 - Pop Shooter & Puzzle Game', 'Sweet Escapes: Design a Bakery with Puzzle Games', '501 Free New Room Escape Game - Mystery Adventure', 'Sudoku Master: Logic puzzle', 'Jellipop Match-Decorate your dream island！', 'Inside Out Thought Bubbles', 'The Moron Test: Challenge Your IQ with Brain Games', 'Jewels of Rome: Gems Puzzle', 'Sudoku.com - сlassic sudoku', 'パズル＆ドラゴンズ(Puzzle & Dragons)', 'You Must Escape', 'Angry Birds Blast', 'Brain Up', 'Brain Dots', 'Dicedom - Merge Puzzle', 'X Construction Lite', 'Jigsaw Puzzles Collection HD - Puzzles for Adults', 'Art Puzzle - picture art games', 'Sudoku - Classic Brain Puzzle Game', 'Find the difference 750 + levels', '애니팡3', 'Witch Puzzle - Magic Match 3', 'Bejeweled Stars – Jewel Match 3', 'Tile Fun - Classic Puzzle Game', 'Pokémon Café ReMix', 'Beyblade Burst Rivals', 'Find the Difference 1000+ levels', 'Physics Drop', 'Brain Training', 'Connection - Stress Relief', 'Cookie Jam Blast™ Match 3 Game', 'Chamy - color by number', 'Forest Rescue: Match 3 Puzzle', 'Find the differences', 'Hidden Objects House Cleaning – Rooms Clean Up', 'Blockudoku®: block puzzle game', 'Cradle of Empire Egypt Match 3', 'Adventure Escape: Time Library', 'Paper Fold', 'Block Puzzle', 'MonsterBusters: Match 3 Puzzle', 'Clockmaker: Match 3 Games!', 'Escape game : 50 rooms 1', 'Friday the 13th: Killer Puzzle', 'Lollipop: Sweet Taste Match 3', 'Gears logic puzzles', 'TripTrap', 'Bad Piggies HD', 'Puzzlerama - Lines, Dots, Blocks, Pipes & more!', 'Tangram HD', 'Color Roll 3D', 'Tebak Gambar', 'Wood Block Puzzle', 'Line Puzzle: String Art', 'EverMerge: Merge 3 Puzzle', 'The Room Two', 'Stencil Art - Spray Masters', 'Love Balls', 'Bắt Chữ - Duoi Hinh Bat Chu', 'Go Knots 3D', 'That level again 2', 'Hello Cats', 'Lily’s Garden', 'ضربة معلم - لعبة الغاز مسلية', 'Railway bridge - build bridges', 'Little Alchemy', 'Scribblenauts Remix', 'Home Design : Hawaii Life', 'Spotlight: Room Escape', 'Jigsaw Puzzle World', 'AMAZE!', 'Fun Escape Room - Mind puzzles', 'Kazı Kazan', 'Juice Cubes', 'My Block', 'Mekorama', 'Plumber 3', 'Find Objects', '100 Doors 2013', 'Sweet Fruit Candy', 'Block Puzzle', 'Color Puzzle Game - Hue Color Match Offline Games', 'Pocket World 3D', 'Happy Glass', '1LINE - one-stroke puzzle game', 'Bitcoin Blast - Earn REAL Bitcoin!', 'Mahjong King', '디즈니 틀린그림찾기', 'Jewel Hunter Lost Temple', \"Where's My Water?\", 'Pirate Treasures - Gems Puzzle', 'Easy Game - brain test', 'Escape the Prison - Adventure Game', 'The Wizard of Oz Magic Match 3', 'Jigsaw Puzzles Epic', 'Clue Hunter', 'Tiny Room Stories: Town Mystery', 'Can You Escape', 'クラッシュフィーバー：パズルRPGで4人協力マルチプレイ！', 'Bubble CoCo : Bubble Shooter', 'Traffic Puzzle - Match 3 Game', 'А4 Чатик', 'Infinity Loop - Simply Relax', 'Cut the Rope: Experiments', 'Water Splash - Cool Match 3', 'Monument Valley', 'Jail Breaker: Sneak Out!', 'Mahjong', 'Merge Magic!', 'Cross Stitch', 'Dropdom - Jewel Blast', 'Gems or jewels ?', 'Disney Frozen Adventures', 'Adventure Escape: Murder Manor', 'Gummy Drop! Match 3 to Build', 'Park Master', 'Imposter Solo Kill', 'Classic Labyrinth 3d Maze', 'Construction City', 'Rescue Cut - Rope Puzzle', 'Empires & Puzzles: Match-3 RPG', 'Gems of War - Match 3 RPG', 'Bubble Shooter - Snoopy POP!', 'Fruits Legend', 'Sugar Smash: Book of Life', 'Unblock Me FREE', 'Roll the Ball® - slide puzzle', 'Troll Face Quest: Horror', 'Wize levels- hack your logic', 'Nonogram.com - picture cross', 'Crash Fever', 'Can You Escape 2', '프렌즈팝', 'Doodle Alchemy', 'True Color', 'Cookie Jam™ Match 3 Games', 'Tasty Tale: puzzle cooking game', 'Cascade: Jewel Matching Adventure', 'Gallery: Coloring Book & Decor', 'Comics Bob', 'PixNite - Color by number', 'Mystery Match - Puzzle Match 3', 'Sand Balls - Puzzle Game', 'Fruits Mania : Elly’s travel', 'Mahjong Solitaire: Classic', 'Train Taxi', 'Orbit - Playing with Gravity', 'Merge Dragons!', 'Angry Birds Match 3', 'Sudoku Fun', 'Word Search Puzzles', 'My Home Design Story : Episode Choices', 'Bubble Shooter - Home Design', 'Bubble Shooter ™', 'Bad Piggies', 'PicWords™', 'Chigiri: Paper Puzzle', 'That Level Again 3', 'Bejeweled Blitz', 'Brainzzz', 'Brain Wars', 'Penny & Flo: Finding Home', 'Pudding Monsters', 'Triple Town', 'Block puzzle', 'Bubble Witch 3 Saga', 'Bus Parking 3D', 'Smurfs Bubble Shooter Story', 'Spy Ninja Network - Chad & Vy', 'Bubble Blast 2', 'Super 2048', 'Can You Escape - Tower', 'Crazy Dino Park', 'Fruit Land match 3 for VK', 'Hello Stars', 'Lazors', 'Find The Differences - The Detective', 'Interlocked', 'Escape Titanic', 'Alchemy Classic HD', '2048', 'Angry Birds Dream Blast', 'Escape game: 50 rooms 3', 'Balls Bricks Breaker', 'Troll Face Quest: Sports Puzzle', 'Jigsaw Puzzles - puzzle games', 'I Love Hue', 'Cannon Shot!', 'Languinis: Word Game', '100 Doors Challenge', 'RGB Express', '100 Gates', 'Bouncy Ball', 'Horror Escape', 'Gemmy Lands: Match 3 Games', 'Unblock Ball - Block Puzzle', 'Jigsaw Puzzle - Classic Puzzle Games', 'Symmetry: Path to Perfection', 'Lost Jewels - Match 3 Puzzle', 'Unroll Me ™- unblock the slots', 'Looper!', 'Plumber', 'Cut It: Brain Puzzles', 'Jewels Temple', 'Lemmings', 'Marble Woka Woka: Jungle Blast', 'Can you escape the 100 room XI', '4 Pics 1 Song', 'Move the Block : Slide Puzzle', 'Ark of War - Dreadnought', 'War Commander: Rogue Assault', 'Lapse: A Forgotten Future', 'Dawn of Titans: War Strategy RPG', 'Guns of Glory: The Iron Mask', 'Seven Guardians', 'Age of Ottoman', 'METAL SLUG DEFENSE', 'War and Magic: Kingdom Reborn', 'Defense Zone 3 HD', 'Castle Clash: Lonca Mücadelesi', 'Clash of Kings', 'Dragons of Atlantis', 'Clash of Empire: Strategic Empire Age', 'Defender II', 'Battle for the Galaxy', 'Stickman Battle 2021: Stick Fight War', \"Guns'n'Glory WW2\", 'X-War:Clash of Zombies', 'Puzzles & Survival', 'Clash of Lords 2: Guild Castle', 'Plants vs. Zombies FREE', 'Lords Mobile: Tower Defense', 'West Game', 'State of Survival: The Zombie Apocalypse', 'Kingdom Wars - Tower Defense Game', 'Realm Defense: Hero Legends TD', 'Dungeon Keeper', 'Empire:Rome Rising', 'Fort Conquer', 'Tribal Wars', 'Clash of Lords 2: Español', 'Galaxy Control: 3D strategy', 'Trench Assault', 'Robotek', 'Sea Port: Manage Ship Tycoon', 'Farm Clan Farm Life Adventure', 'Mafia City', 'Last Shelter: Survival', 'Clash of Clans', 'Empire Z: Endless War', 'Little Empire', 'Chef Rescue: Restaurant Tycoon', 'Art of War: Legions', 'Clash of Lords 2: Clash Divin', 'War Troops 1917: Trench Warfare WW1 Strategy Game', 'Stick War: Legacy', 'Throne Rush', 'Gods of Olympus', 'Little Commander - WWII TD', 'Virtual City Playground: Build', 'Hackers', 'Castle Crush：Epic Battle', 'Tactile Wars', 'Idle Construction 3D', 'Random Dice: PvP Defense', 'Kingdom Rush - Tower Defense', 'Iron Desert - Fire Storm', 'Days of Empire - Heroes Never Die!', 'Mobile Royale MMORPG - Build a Strategy for Battle', 'aa', 'Castle Clash: Regu Royale', \"Castle Clash: King's Castle DE\", 'Castle Clash: Gilda Reale', 'My Ice Cream Truck:🍧Make Sweet Frozen Desserts🍦', 'Deep Town: Idle Mining Tycoon', \"King's Bounty Legions: Turn-Based Strategy Game\", 'Grow Empire: Rome', 'Kiss of War', 'Grepolis - Divine Strategy MMO', \"The Walking Dead No Man's Land\", 'Last Empire - War Z: Strategy', 'Command & Conquer: Rivals™ PVP', 'King of Avalon: Dominion', 'Dota Underlords', 'انتقام السلاطين', \"Five Nights at Freddy's AR: Special Delivery\", 'Dead Ahead: Zombie Warfare', 'Badland Brawl', 'League of Legends: Wild Rift', 'Deck Heroes: Великая Битва!', 'Vlogger Go Viral: Streamer Tuber Life Simulator', \"Evony: The King's Return\", 'Castle Creeps - Tower Defense', 'Castle Clash: Quyết Chiến-Gamota', 'World at War: WW2 Strategy MMO', 'Dawn of Steel', '城堡爭霸 - 聯盟霸業', 'Space Arena: Spaceship games - 1v1 Build & Fight', 'World at Arms', 'War Dragons', 'Dino Bash - Dinosaurs v Cavemen Tower Defense Wars', 'War Heroes: Strategy Card Game', 'Tower Conquest: Tower Defense Strategy Games', 'Venom Angry Crashy Rush Online', 'Boom Beach', 'MARVEL Super War', 'Дикий запад: Новые земли', 'Cat War2', 'TRANSFORMERS: Earth Wars', 'Rise of the Kings', 'Clash of Lords 2: Битва Легенд', 'Castle Clash: ลีกขั้นเทพ', 'VEGA Conflict', 'Castle Clash: حرب التحالفات', 'Z Day: Hearts of Heroes | MMO Strategy War', \"King's Empire\", 'The Walking Dead: Survivors', 'Zombie Defense', 'Top War: Battle Game', '1941 Frozen Front', 'Pirates of the Caribbean: ToW', 'Bloons TD 5', 'Gladiator Heroes of Kingdoms', 'State.io - Conquer the World', 'War and Order', 'Bid Wars 2: Auction & Business', 'Age of War', 'Warhammer 40,000: Space Wolf', 'Emoji Craft', 'Chess Live', 'Zgirls', 'March of Empires: War of Lords', 'Toilet Time: Fun Mini Games', 'Spartania: The Orc War! Strategy & Tower Defense!', 'Unblock Car', 'Empire: Four Kingdoms', 'Rise of Kingdoms: Lost Crusade', 'Game of Warriors', 'Cell Expansion Wars', 'World Conqueror 3-WW2 Strategy', 'Monster Legends', 'Disney Heroes: Battle Mode', 'Pocket Ants: Colony Simulator', 'Warlords of Aternum', 'South Park: Phone Destroyer™ - Battle Card Game', 'INVASION: صقور العرب\\u200e', 'Clash Royale', 'Empire Warriors TD Offline', 'Imperia Online - Medieval empire war strategy MMO', 'Conquerors: Golden Age', 'ICE - Minimalistic Strategy', 'EMERGENCY HQ - firefighter rescue strategy game', 'The Godfather: Family Dynasty', 'Kingdom Rush Frontiers TD', 'Vikings: War of Clans – MMO', 'Onmyoji Arena', 'Tower Defense King', 'Galaxy Defense (Tower Game)', 'Clash of Lords 2: Ehrenkampf', 'King of Thieves', 'Bloons TD 6', 'TFT: Teamfight Tactics', 'Clash of Lords: Guild Castle', 'Bid Wars - Auction Simulator', 'Defenders: TD Origins', 'Steampunk Defense: Tower Defense', 'Battle of Polytopia - A Civilization Strategy Game', 'Reactor - Energy Sector Tycoon', 'Ships of Battle - Age of Pirates - Warship Battle', 'My Virtual Pet Shop Care Games', 'Battle Islands', 'Supermarket Mania Journey', 'World of Tanks Blitz', 'Empire Warriors: Tower Defense Offline Game', 'Total Conquest', 'Megapolis: city building simulator. Urban strategy', \"Summoner's Greed: Idle TD Hero\", 'Smashy City - Destruction Game', 'Mini Warriors', 'Evo Pop', 'Age of Kings: Skyward Battle', 'Tentacle Wars ™', 'Warlings: Armageddon', 'Rise of Empires: Ice and Fire', 'Star Trek™ Fleet Command', 'Smashing Four', 'Stormfall: Rise of Balur', 'Alien Creeps - Tower Defense', 'Chess Rush', 'Brutal Age: Horde Invasion', 'Castle Clash: Dominio del Reino', 'METAL SLUG ATTACK', 'Forge of Empires: Build a City', 'Train Station 2: Tycoon Games', 'War of Nations: PvP Strategy', 'Castle Clash: Guild Royale', '2020: My Country', 'Rush Royale - Tower Defense TD', 'Arknights', 'Jungle Heat: War of Clans', 'Radiant Defense', 'What the Hen: 1on1 summoner game', 'Art of War 3:RTS strategy game', 'Empires and Allies', 'Warpath', 'Army Men Strike: Toy Wars', 'Cooking Frenzy®️Cooking Game', 'Castle Clash: Batalha de Guildas', 'Pocket Tanks', 'DomiNations', 'CastleStorm - Free to Siege', 'Olympus Rising: Tower Defense and Greek Gods', 'Bloons TD Battles', 'Castle Clash : Guild Royale', 'Last Hope TD - Zombie Tower Defense Games Offline', 'Castle Clash: Схватка Гильдий', 'Biotix: Phage Genesis', 'Kingdom Rush Origins - TD', 'European War 4 : Napoleon', 'Auto Chess', \"Baldi's Basics Classic\", 'Serious Game [Multiplayer Zombie Shooter Survival]', 'Help! The Serious Game', 'OIE Rinderpest Serious Game', 'Otsimo | Special Education Autism Learning Games', 'Prodigy Math Game', 'Adventure Academy', 'Toddler puzzle games for kids - Match shapes game', 'Journeys: Interactive Series', 'Love Sick: Love Stories Games', 'Leisure Suit Larry: Reloaded - 80s and 90s games!', 'Love Choice: Interactive game, new story & episode', 'PBS KIDS Games', 'Life Choices: Simulation Game', 'Episode - Choose Your Story', 'Game of Thrones: Conquest ™ - Strategy Game', 'Psych! Outwit your friends', 'Best Fiends - Match 3 Puzzles', 'Chapters: Interactive Stories', 'A Blind Legend', 'Injection Doctor Emergency Hospital : Doctor Games', 'Clockwork Brain Training - Memory & Attention Game', 'Wheel of Fortune: TV Game', 'The Wolf Among Us', 'Kharaboo Wars: Orcs assault', 'Archer: Danger Phone Idle Game', 'Reigns: Game of Thrones', 'SHIN MEGAMI TENSEI L Dx2', 'Plague Inc.', 'Fire Emblem Heroes', 'I am innocent 🔎 Crime investigation Mystery games', 'Project RIP Mobile - Free Horror Survival Shooter', 'Pocket Build - Unlimited open-world building game', 'Xbox Game Pass', 'GUNDAM BREAKER MOBILE', 'Cobra Kai: Card Fighter', '911 Operator', 'ANOTHER EDEN The Cat Beyond Time and Space', 'Life on Earth: evolution game', 'Habitica: Gamify Your Tasks', 'Teach Your Monster to Read: Phonics & Reading Game', 'Memokids: Memory game for kids', 'Magic Land ADHD - Learning School Tasks By Playing', 'CogniFit - Test & Brain Games', 'Egg, Inc.', 'Mightier', 'My Gym: Fitness Studio Manager', 'Brili Routines – ADHD Habit Tracker', 'Teladoc | Telehealth & Therapy', 'Pixicade', 'Math Kids - Add, Subtract, Count, and Learn', 'Braindoku: Sudoku Block Puzzle', 'Crayola Create & Play: Coloring & Learning Games', 'Jungle Animal Hair Salon - Styling Game for Kids', 'Speechify - text to speech tts', 'Battle Chasers: Nightwar', 'AutiSpark: Kids Autism Games', 'Neuriva Brain Gym', 'Brain Games Kids', 'AntiStress, Relaxing, Anxiety & Stress Relief Game', 'Amaru: The Self-Care Pet', 'My Oasis: Calming, Relaxing & Anxiety Relief Game', 'GoNoodle - Kids Videos', 'Bonza Word Puzzle', 'Crossword Jam', 'Tiny Worlds: Dragon Idle games', 'Fidget Spinner', 'Healthy Minds Program', 'Bored Button - Games', 'Brain School: Brain Games', 'KAMI 2', 'GoNoodle Games - Fun games that get kids moving', 'Poptropica', 'ADHD - Cognitive Research', \"I'm On It: Focus Timer for ADHD & ASD\", 'ADHD  - Attention deficit hyperactivity disorder', 'Inflow ADHD', 'Fidget toys stress reliever! Anti anxiety (ADHD)', 'Engross: Focus Timer, To-Do List & Day Planner', 'ADHD Self Test', 'Memory & Attention Training for Kids', 'ADHD Mantra', 'ADHDecoder - Rewire your Brain', 'ADHD Health Storylines', 'Do It Now: RPG To Do List. Habit Tracker. Planner', 'MindPal - Brain Training', 'Dyslexia - Cognitive Research', 'Train your Brain']\n"]}],"source":["def getFieldData(fieldName, db):\n","  result = [r[fieldName] for r in db]\n","  return result\n","\n","game_names = getFieldData('title', db)\n","print('Game names: \\n', game_names)"]},{"cell_type":"markdown","metadata":{"id":"ACYy5Cczd-wX"},"source":["Since the game names are not very suitable for querying we need to preprocess them. Firstly, they usually contain more words and we can notice two main parts of the name (one before signs **' - '** or **' : '**, another after them). Some of them are writen in the language different from English. These are some of the examples:\n","\n","* *Learn 33 Languages Free - Mondly*\n","* *Sololearn: Learn to Code (Python, Javascript, etc)*\n","* *U-Dictionary*\n","* *ANTON: Kindergarten - Grade 5*\n","* *Language Learning - Spanish, Korean, French & More*\n","* *Meritnation: CBSE, ICSE & more (Free Live Classes)*\n","* *تعليم اللغة الانجليزية من الصفر بالصوت والصورة*\n","* *wifistudy - #1 Exam Preparation, Free Mock Tests*\n","* *Exam Preparation App: Live Class | Mock Test | PYP*\n","\n","\\\\\n","\n","Since there are names that are not in English, first, we will check the language. If the language is English in that case we can split the name into shorter textual parts. Otherwise, we won't modify the name. Now, we need to decide how to split the names as those parts as queries too."]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":11875,"status":"ok","timestamp":1639411549899,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"Hziq9QbkqD77"},"outputs":[],"source":["queries_dict = {}\n","\n","for name in game_names:\n","\n","  if name not in queries_dict:\n","    queries_dict[name] = []\n","\n","  # First add the whole name\n","  queries_dict[name].append(name)\n","  if name.isnumeric()==True:\n","    continue\n","  if detect(name) != 'en':\n","    queries_dict[name].append(name)\n","  \n","  else:\n","    # Split the name when ' - ' is present\n","    sub_names = name.split(' - ') \n","    queries_dict[name].extend(sub_names)\n","    continue\n","\n","    # Split the name when ':' is present\n","    sub_names = name.split(':') \n","    queries_dict[name].extend(sub_names)\n","    continue\n","\n","    # Split the name when '|' is present\n","    sub_names = name.split('|') \n","    queries_dict[name].extend(sub_names)\n","    continue\n","\n","\n","# Unique list elements\n","for key in queries_dict:\n","  queries_dict[key] = list(set(queries_dict[key]))\n"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1639411549900,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"EEccbUr6c1dZ"},"outputs":[],"source":["def count_occurances_study_type_keywords(abstract, keywords_dict):\n","  \n","  \"\"\" Given one abstract in semi-processed format and the Study type \n","      keywords dictiory previously obtained \"\"\"\n","  \n","  result_dict = {}\n","  counts_total = 0\n","  for study_type in keywords_dict:\n","    counts = 0\n","    for word in keywords_dict[study_type]:\n","      counts += abstract.count(word)\n","    counts_total += counts\n","    result_dict[study_type] = counts\n","\n","  result_dict = {k: round(v/(counts_total+0.01), 4) for k, v in result_dict.items()}\n","\n","  return result_dict"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzGG4AmL8HyW","outputId":"ce43889c-fb69-485e-8383-e18611fb2f99"},"outputs":[{"ename":"FeatureNotFound","evalue":"Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)","\u001b[1;32mC:\\Users\\PHILLI~1\\AppData\\Local\\Temp/ipykernel_17508/3277964608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mqueries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[1;31m# Gather the abstracts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m   \u001b[0mabstracts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjournals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdois\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthorss\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mall_publications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m   \u001b[1;31m# Extract information (ID, Name, Authors, Year, Study type, DOI, keywords)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m   \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Users\\PHILLI~1\\AppData\\Local\\Temp/ipykernel_17508/3000019529.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, query_words)\u001b[0m\n\u001b[0;32m     50\u001b[0m            \u001b[0mauthorss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m            \u001b[0marticle_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPublication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpubmedurlbase\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0marticle_ID\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Users\\PHILLI~1\\AppData\\Local\\Temp/ipykernel_17508/1411654012.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_url)\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Connection refused\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0marticle_abs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mbuilder_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuilder_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 raise FeatureNotFound(\n\u001b[0m\u001b[0;32m    246\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"]}],"source":["from urllib.error import HTTPError\n","\n","dict_game_score = {}\n","num_pubs_per_game = {}\n","dict_game_papers = {}\n","\n","# Go through all the games \n","for name in game_names:\n","  all_publications = PubMedPublications(number_of_articles = 10)\n","  # Take queries for one particular game\n","  queries = queries_dict[name]\n","  # Gather the abstracts\n","  abstracts, IDs, titles, kws, journals, dois, authorss  = all_publications.search(query_words = queries)\n","  # Extract information (ID, Name, Authors, Year, Study type, DOI, keywords)\n","  data = {}\n","  data['ID'] = IDs\n","  data['Title'] = titles\n","  data['Keywords'] = kws\n","  data['Journal'] = journals\n","  data['DOI'] = dois\n","  data['Authors'] = authorss\n","\n","  df = pd.DataFrame.from_dict(data)\n","  dict_game_papers[name] = df\n","\n","  #print(name, len(abstracts))\n","  num_pubs_per_game[name] = len(abstracts)\n","  results_abs = []\n","  for abstract in abstracts:\n","    result = count_occurances_study_type_keywords(abstract.lower(), study_type_dictionary)\n","    results_abs.append(result)\n","  \n","  dict_game_score[name] = results_abs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1639069184233,"user":{"displayName":"ivamil","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7AAJunsulh8FBfUUSpibDxofJfIOiafLYXn0dEg=s64","userId":"13328705101060990327"},"user_tz":-60},"id":"4wzJzyDcofWF","outputId":"7f848258-8e71-406c-bed6-75c09c17cba7"},"outputs":[{"data":{"text/plain":["[{'MetaAnalysis': 0.0714,\n","  'ObservationalStudy': 0.1071,\n","  'Other': 0.0,\n","  'RCT': 0.8211,\n","  'SystematicReview': 0.0},\n"," {'MetaAnalysis': 0.0,\n","  'ObservationalStudy': 0.3633,\n","  'Other': 0.0,\n","  'RCT': 0.6358,\n","  'SystematicReview': 0.0}]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["dict_game_score['100 Doors 2013']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J953JbFBI0bZ"},"outputs":[],"source":["import pickle\n","with open('dict_game_scores.pickle', 'wb') as handle:\n","    pickle.dump(dict_game_score, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open('number_pubs_per_game.pickle', 'wb') as handle:\n","    pickle.dump(num_pubs_per_game, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open('pubs_per_game.pickle', 'wb') as handle:\n","    pickle.dump(dict_game_papers, handle, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","metadata":{"id":"TFiCp19OLdCs"},"source":["# **INFORMATION EXTRACTION**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1h7WbeikqEC3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4X0woa3SLiKv"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNueg0yqJJwWM1/qYW4dbwF","collapsed_sections":[],"name":"EH_Project.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}
